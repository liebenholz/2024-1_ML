{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Libraries\n",
        "이번에는 scikit-learn 말고도 Gradient Boosting 기반의 다른 알고리즘들을 사용해보자."
      ],
      "metadata": {
        "id": "pSdr4TGRbAvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8-SMDK0azoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e086f2d5-d651-4a03-c333-3df72aa5127f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q scikit-learn==1.4.1.post1 numpy pandas altair xgboost lightgbm catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [xgboost](https://xgboost.ai/): XGBoost\n",
        "* [lightgbm](https://lightgbm.readthedocs.io/en/stable/): LightGBM\n",
        "* [catboost](https://catboost.ai/): CatBoost"
      ],
      "metadata": {
        "id": "dS5V_mDFvLFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "\n",
        "지난 시간과 마찬가지로, **breast_cancer** 데이터셋을 활용해보자."
      ],
      "metadata": {
        "id": "U3Y453MQbXPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "I_train, I_test = next(StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42).split(X, y))\n",
        "X_train, y_train = X[I_train], y[I_train]\n",
        "X_test, y_test = X[I_test], y[I_test]"
      ],
      "metadata": {
        "id": "qTfHHb8GAPXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest\n",
        "일단, Baseline으로 Bagging + Random Subspace인 Random Forest를 훈련시킬것이다."
      ],
      "metadata": {
        "id": "JfE06w2Av_Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=100, # 100개의 Decision Tree를 만든다\n",
        "    max_depth=5, # 각 Decision Tree의 깊이는 5로 제한한다\n",
        "    max_features='sqrt', # 30개의 특성값 공간 중 약 5개만 사용한다\n",
        "    max_samples=0.5, # 전체 훈련 샘플 중 50%만 Bootstrap한다\n",
        "    random_state=42\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "SYXfkFXkwKtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- Random Forest: ', accuracy_score(y_train, model_rf.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- Random Forest: ', accuracy_score(y_test, model_rf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFzE174twNVQ",
        "outputId": "ad3e6d69-d434-4d2c-b6bb-d3226c1dc578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- Random Forest:  0.9882697947214076\n",
            "# Validation performance\n",
            "- Random Forest:  0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost\n",
        "\n",
        "먼저 AdaBoost부터 시험해보자. 구현체는 [sklearn.ensemble.AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)에 있다."
      ],
      "metadata": {
        "id": "ZT_Y_O_fJ4K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "model_ab = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42), # 깊이 1의 Decision Tree, 즉 Stump를 활용한다. 기본값이 이렇게 설정되어 있다.\n",
        "    n_estimators=100, # 최대 100개의 Stump를 만들 것이다. 하지만, 중간에 Loss가 0가 된다면, 더 이상 Stump를 생성하지 않고 훈련이 종료된다.\n",
        "    learning_rate=1.0, # 각 Stump가 미치는 영향을 나타낸다. 일단은 기본값으로 설정하자.\n",
        "    random_state=42\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "tPt915NbMSmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b25f890-e614-4598-9911-baec53b0b875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- AdaBoost: ', accuracy_score(y_train, model_ab.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- AdaBoost: ', accuracy_score(y_test, model_ab.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLS1UVu8xIue",
        "outputId": "339436bb-28f3-4a5f-d365-5c5b57af0e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- AdaBoost:  1.0\n",
            "# Validation performance\n",
            "- AdaBoost:  0.9605263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다행스럽게도(?) Random Forest보다 훨씬 성능이 좋은 것을 알 수 있다."
      ],
      "metadata": {
        "id": "JJ0fqBw9xQFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting\n",
        "이번엔, Gradient Boosting을 활용해보자. 기본적인 Gradient Boosting 알고리즘은 구현체는 [sklearn.ensemble.GradientBoostingClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)에 구현되어 있다."
      ],
      "metadata": {
        "id": "cTqzCm8SxU8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "model_gb = GradientBoostingClassifier(\n",
        "    n_estimators=100, # 최대 100개의 Decision Tree를 만들 것이다. 하지만, 중간에 Loss가 0가 된다면, 더 이상 Decision Tree를 생성하지 않고 훈련이 종료된다.\n",
        "    learning_rate=0.1, # 각 Decision Tree가 결과에 미치는 영향을 나타낸다(Shrinkage). 과대적합을 막기 위해서 낮은 값으로 설정해보자\n",
        "    subsample=0.8, # 각 Decision Tree 훈련에 사용할 샘플의 개수를 나타낸다. 80%만 활용해보자.\n",
        "    max_depth=3, # Decision Tree의 깊이를 나타낸다. 많이 깊어지지 않도록 해보자.\n",
        "    min_samples_leaf=1, # Decision Tree의 잎 노드에 필요한 샘플의 개수를 나타낸다. 깊이를 조절했으므로, 이건 그냥 1로 놔두자.\n",
        "    max_features=1.0, # Random Subspace의 적용을 나타낸다. 전체 Feature를 다 사용하자.\n",
        "    random_state=42\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "EYcjPfPixwsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_train, model_gb.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_test, model_gb.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nxe5isYM-lq",
        "outputId": "8a041297-c211-4490-d97a-89200785befb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- Gradient Boosting:  1.0\n",
            "# Validation performance\n",
            "- Gradient Boosting:  0.9605263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost와 같은 성능이 나왔다."
      ],
      "metadata": {
        "id": "XB9O2rHwHHBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n",
        "이번엔 Gradient Boosting 기반의 응용 알고리즘들을 사용해보자. 먼저, XGBoost다.\n",
        "\n",
        "초기 XGBoost는 자신들만의 데이터 구조(예. [xgboost.DMatrix](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix)) 및 훈련 함수(예. [xgboost.train](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.train)) 등이 있었으나, 이제는 scikit-learn과 동일한 문법을 통해 XGBoost 모델을 훈련시킬 수 있도록 지원한다. 해당 구현체는 [xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)에서 확인할 수 있다."
      ],
      "metadata": {
        "id": "NdiK2SrxNlcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=100, # 최대 100개의 Decision Tree를 만들 것이다. 하지만, 중간에 Loss가 0가 된다면, 더 이상 Decision Tree를 생성하지 않고 훈련이 종료된다.\n",
        "    learning_rate=0.1, # 각 Decision Tree가 결과에 미치는 영향을 나타낸다(Shrinkage). 과대적합을 막기 위해서 낮은 값으로 설정해보자\n",
        "    subsample=0.8, # 각 Decision Tree 훈련에 사용할 샘플의 개수를 나타낸다. 80%만 활용해보자.\n",
        "    max_depth=3, # Decision Tree의 깊이를 나타낸다. 많이 깊어지지 않도록 해보자.\n",
        "    random_state=42\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "5rcwwEVhNt6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_train, model_xgb.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_test, model_xgb.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcba4YU2NzIp",
        "outputId": "5bfd2e54-f7e5-436b-b07b-ebffad893e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- Gradient Boosting:  1.0\n",
            "# Validation performance\n",
            "- Gradient Boosting:  0.9692982456140351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 확인한 것 중 가장 좋은 성능이 나왔다.\n",
        "\n",
        "위에서 설정한 초매개변수 이외에도, XGBoost에는 정말 많은 초매개변수가 존재한다. 보다 자세한 건 [공식 문서](https://xgboost.readthedocs.io/en/stable/parameter.html)를 확인하자."
      ],
      "metadata": {
        "id": "Y462c8mpJNLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM\n",
        "이번엔 LightGBM이다. 역시 LightGBM도 XGBoost와 유사하게 자신만의 인터페이스가 있었으나, scikit-learn과 동일한 인터페이스 또한 지원한다. LightGBM 기반 분류기는 [lightgbm.LGBMClassifier](https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.LGBMClassifier.html#)에 구현되어 있다."
      ],
      "metadata": {
        "id": "qH9mR2vzN3uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "model_lgbm = LGBMClassifier(\n",
        "    n_estimators=100, # 최대 100개의 Decision Tree를 만들 것이다. 하지만, 중간에 Loss가 0가 된다면, 더 이상 Decision Tree를 생성하지 않고 훈련이 종료된다.\n",
        "    learning_rate=0.1, # 각 Decision Tree가 결과에 미치는 영향을 나타낸다(Shrinkage). 과대적합을 막기 위해서 낮은 값으로 설정해보자\n",
        "    subsample=0.8, # 각 Decision Tree 훈련에 사용할 샘플의 개수를 나타낸다. 80%만 활용해보자.\n",
        "    max_depth=3, # Decision Tree의 깊이를 나타낸다. 많이 깊어지지 않도록 해보자.\n",
        "    random_state=42,\n",
        "    verbosity=-1\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "MenoTS22KPG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_train, model_lgbm.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_test, model_lgbm.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABkQxoSXKZKx",
        "outputId": "24ad6fa7-4ec9-4c5f-a95b-5576c03737fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- Gradient Boosting:  1.0\n",
            "# Validation performance\n",
            "- Gradient Boosting:  0.9605263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "역시, 위에서 설정한 초매개변수 이외에도, LightGBM에는 정말 많은 초매개변수가 존재한다. 보다 자세한 건 [공식 문서](https://lightgbm.readthedocs.io/en/latest/Parameters.html)를 확인하자"
      ],
      "metadata": {
        "id": "n8UPxxMmOHR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoost\n",
        "마지막으론 CatBoost이다. CatBoost 또한 scikit-learn과 동일한 인터페이스를 활용해서 모델을 훈련할 수 있다. 구현체는 [catboost.CatBoostClassifier](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)다."
      ],
      "metadata": {
        "id": "tuct39xOK0oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "model_cat = CatBoostClassifier(\n",
        "    n_estimators=100, # 최대 100개의 Decision Tree를 만들 것이다. 하지만, 중간에 Loss가 0가 된다면, 더 이상 Decision Tree를 생성하지 않고 훈련이 종료된다.\n",
        "    learning_rate=0.1, # 각 Decision Tree가 결과에 미치는 영향을 나타낸다(Shrinkage). 과대적합을 막기 위해서 낮은 값으로 설정해보자\n",
        "    subsample=0.8, # 각 Decision Tree 훈련에 사용할 샘플의 개수를 나타낸다. 80%만 활용해보자.\n",
        "    max_depth=3, # Decision Tree의 깊이를 나타낸다. 많이 깊어지지 않도록 해보자.\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ").fit(\n",
        "    X_train, y_train\n",
        ")"
      ],
      "metadata": {
        "id": "QamXMefeLD_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print('# Training performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_train, model_cat.predict(X_train)))\n",
        "\n",
        "print('# Validation performance')\n",
        "print('- Gradient Boosting: ', accuracy_score(y_test, model_cat.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqoLdyN2Lh4r",
        "outputId": "938e3362-9f32-4e7d-961f-b968267e3959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Training performance\n",
            "- Gradient Boosting:  1.0\n",
            "# Validation performance\n",
            "- Gradient Boosting:  0.9605263157894737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "반복되는 이야기지만, CatBoost 또한 아주 많은 초매개변수가 존재한다. 보다 자세한 건 [공식 문서](https://catboost.ai/en/docs/references/training-parameters/)를 확인하자"
      ],
      "metadata": {
        "id": "F_AgJk1nLmvp"
      }
    }
  ]
}